{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fcf1ef-e780-44a1-a438-50dcc979c48d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urls = [\n",
    "    \"https://datasets.imdbws.com/title.ratings.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.basics.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.principals.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.crew.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.akas.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.episode.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/name.basics.tsv.gz\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    filename = \"/Volumes/workspace/imdb_project/filestore/\" + url.split(\"/\")[-1]\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3179efa-81b4-4df8-8363-bcd44bc4310b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "src_path = \"/Volumes/workspace/imdb_project/filestore/\"\n",
    "dst_path = \"/Volumes/workspace/imdb_project/bronze/\"\n",
    "\n",
    "# create bronze folder if not exists\n",
    "dbutils.fs.mkdirs(dst_path)\n",
    "\n",
    "files = [f.name for f in dbutils.fs.ls(src_path) if f.name.endswith(\".gz\")]\n",
    "\n",
    "for file in files:\n",
    "    gz_file = src_path + file\n",
    "    tsv_file = dst_path + file.replace(\".gz\", \"\")\n",
    "\n",
    "    with gzip.open(gz_file.replace(\"dbfs:\", \"\"), 'rb') as f_in:\n",
    "        with open(tsv_file.replace(\"dbfs:\", \"\"), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Unzipping complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f261f0-a2ca-4736-a592-ec37d24d966d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ydata-profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05e7e82-5925-4670-9caa-64d898a03433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS imdb_project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f67db2f-96e7-49a2-acf5-4d2db64028d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS imdb_project_bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS imdb_project_silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS imdb_project_rejects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf761b87-9a22-4c58-86cc-57449e959779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE imdb_project_bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ded875d-b531-472c-baf9-8ed7959440c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT current_catalog(), current_database()\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80a372b-1ec2-4bbe-8c62-5cf509530ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "path = \"/Volumes/workspace/imdb_project/bronze/name.basics.tsv\"\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"name.basics.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.name_basics\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.name_basics\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c266652-8023-4f39-89c6-a6fd356ad8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.name_basics\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d5863c-7e5f-43de-9a10-7448b3f71fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df = spark.table(\"workspace.imdb_project_bronze.name_basics\") \\\n",
    "                 .limit(100000) \\\n",
    "                 .toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0ac27f-a119-4ca6-a5fa-75353af1092e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(sample_df, \n",
    "                        title=\"IMDb â€“ name.basics Profiling Report\",\n",
    "                        explorative=True)\n",
    "\n",
    "profile.to_file(\"/Workspace/Shared/imdb_name_basics_profile.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ba30af-dfab-4ff0-ab3f-4b809769bd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/imdb_project/bronze/title.basics.tsv\"\n",
    "\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"title.basics.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.title_basics\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.title_basics\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a6deb2-f9ac-40f4-a9a9-1d64cd51f7b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_basics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b0bc0f-cf0f-49e7-8194-4d58511a211e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/imdb_project/bronze/title.akas.tsv\"\n",
    "\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"title.akas.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.title_akas\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.title_akas\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f400744-72d5-4027-b678-a487f068e86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_akas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ca415a-7cde-4917-8c14-347d6e6ba773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/imdb_project/bronze/title.crew.tsv\"\n",
    "\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"title.crew.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.title_crew\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.title_crew\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af67971-65c0-4cf9-97a7-0967fe2ce88b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/imdb_project/bronze/title.episode.tsv\"\n",
    "\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"title.episode.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.title_episode\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.title_episode\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28acf2c9-0bdc-4060-abee-943961fd5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/imdb_project/bronze/title.principals.tsv\"\n",
    "\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"title.principals.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.title_principals\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.title_principals\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fdfae91-adfa-453b-bdbd-a6f01746d57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/imdb_project/bronze/title.ratings.tsv\"\n",
    "\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"nullValue\", \"\\\\N\")\n",
    "        .csv(path)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"source_file\", F.lit(\"title.ratings.tsv\")) \\\n",
    "       .withColumn(\"load_datetime\", F.current_timestamp())\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.imdb_project_bronze.title_ratings\")\n",
    "\n",
    "display(spark.table(\"workspace.imdb_project_bronze.title_ratings\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abd0425-3a0b-4cca-aec1-274c3af43def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Dictionary of IMDb Bronze tables\n",
    "datasets = {\n",
    "    \"title_basics\": \"workspace.imdb_project_bronze.title_basics\",\n",
    "    \"title_akas\": \"workspace.imdb_project_bronze.title_akas\",\n",
    "    \"title_crew\": \"workspace.imdb_project_bronze.title_crew\",\n",
    "    \"title_episode\": \"workspace.imdb_project_bronze.title_episode\",\n",
    "    \"title_principals\": \"workspace.imdb_project_bronze.title_principals\",\n",
    "    \"title_ratings\": \"workspace.imdb_project_bronze.title_ratings\"\n",
    "}\n",
    "\n",
    "for name, table in datasets.items():\n",
    "    print(f\"Profiling {name} ...\")\n",
    "\n",
    "    # Sample data to avoid overwhelming cluster\n",
    "    pdf = (\n",
    "        spark.table(table)\n",
    "        .limit(100000)\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    profile = ProfileReport(\n",
    "        pdf,\n",
    "        title=f\"IMDb {name} Profiling Report\",\n",
    "        explorative=True\n",
    "    )\n",
    "\n",
    "    # Save report\n",
    "    output_path = f\"/Workspace/Shared/{name}_profile.html\"\n",
    "    profile.to_file(output_path)\n",
    "\n",
    "    print(f\"âœ” Saved: {output_path}\\n\")\n",
    "\n",
    "print(\"ALL REPORTS GENERATED!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56a4248-90b5-4558-b383-76700a4de4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze = spark.table(\"workspace.imdb_project_bronze.name_basics\")\n",
    "\n",
    "# 1. Rows with null nconst -> reject\n",
    "reject_null_key = bronze.filter(F.col(\"nconst\").isNull()) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"nconst is NULL\"))\n",
    "\n",
    "reject_null_key.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.name_basics_null_nconst\"\n",
    ")\n",
    "\n",
    "# 2. Cast years, mark invalid years\n",
    "clean = bronze.filter(F.col(\"nconst\").isNotNull()) \\\n",
    "    .withColumn(\"birthYear_int\", F.col(\"birthYear\").cast(\"int\")) \\\n",
    "    .withColumn(\"deathYear_int\", F.col(\"deathYear\").cast(\"int\")) \\\n",
    "    .withColumn(\"invalid_birth_year\",\n",
    "                (F.col(\"birthYear_int\") < 1850) | (F.col(\"birthYear_int\") > 2030)) \\\n",
    "    .withColumn(\"invalid_death_year\",\n",
    "                (F.col(\"deathYear_int\") < 1850) | (F.col(\"deathYear_int\") > 2035))\n",
    "\n",
    "# 3. Send rows with crazy years to rejects\n",
    "reject_years = clean.filter(F.col(\"invalid_birth_year\") | F.col(\"invalid_death_year\")) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"Birth/Death year out of range\"))\n",
    "\n",
    "reject_years.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.name_basics_bad_years\"\n",
    ")\n",
    "\n",
    "# 4. Keep only valid rows, drop helper flags\n",
    "silver = clean.filter(~(F.col(\"invalid_birth_year\") | F.col(\"invalid_death_year\"))) \\\n",
    "    .drop(\"invalid_birth_year\", \"invalid_death_year\") \\\n",
    "    .withColumn(\"birthYear_clean\", F.col(\"birthYear_int\")) \\\n",
    "    .withColumn(\"deathYear_clean\", F.col(\"deathYear_int\")) \\\n",
    "    .drop(\"birthYear_int\", \"deathYear_int\")\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.name_basics\"\n",
    ")\n",
    "\n",
    "print(\"Silver name_basics rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.name_basics\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7412d5-c4ee-44de-8104-b40f1e391af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze = spark.table(\"workspace.imdb_project_bronze.title_basics\")\n",
    "\n",
    "# Reject null tconst\n",
    "reject_null_key = bronze.filter(F.col(\"tconst\").isNull()) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"tconst is NULL\"))\n",
    "\n",
    "reject_null_key.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_basics_null_tconst\"\n",
    ")\n",
    "\n",
    "clean = bronze.filter(F.col(\"tconst\").isNotNull()) \\\n",
    "    .withColumn(\n",
    "        \"startYear_int\",\n",
    "        F.when(F.col(\"startYear\").rlike(\"^[0-9]+$\"), F.col(\"startYear\").cast(\"int\")).otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"endYear_int\",\n",
    "        F.when(F.col(\"endYear\").rlike(\"^[0-9]+$\"), F.col(\"endYear\").cast(\"int\")).otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"runtimeMinutes_int\",\n",
    "        F.when(F.col(\"runtimeMinutes\").rlike(\"^[0-9]+$\"), F.col(\"runtimeMinutes\").cast(\"int\")).otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"isAdult_bool\",\n",
    "        F.when(F.col(\"isAdult\").rlike(\"^[0-9]+$\"), F.col(\"isAdult\").cast(\"int\")).otherwise(0)\n",
    "    ) \\\n",
    "    .withColumn(\"genres_array\",\n",
    "        F.when(F.col(\"genres\").isNotNull(), F.split(F.col(\"genres\"), \",\"))\n",
    "         .otherwise(F.array().cast(\"array<string>\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"invalid_year\",\n",
    "        (F.col(\"startYear_int\") < 1850) | (F.col(\"startYear_int\") > 2030)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"invalid_runtime\",\n",
    "        F.col(\"runtimeMinutes_int\") < 0\n",
    "    )\n",
    "\n",
    "# Reject rows with invalid year/runtime\n",
    "reject_bad = clean.filter(F.col(\"invalid_year\") | F.col(\"invalid_runtime\")) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"Invalid year or runtime\"))\n",
    "\n",
    "reject_bad.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_basics_invalid_values\"\n",
    ")\n",
    "\n",
    "# Keep only valid rows\n",
    "silver = clean.filter(~(F.col(\"invalid_year\") | F.col(\"invalid_runtime\"))) \\\n",
    "    .drop(\"invalid_year\", \"invalid_runtime\")\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.title_basics\"\n",
    ")\n",
    "\n",
    "print(\"Silver title_basics rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.title_basics\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab865140-11ad-4318-8b14-df71b6b8a0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_basics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415771d3-9a4a-4325-b21b-a25d5bbb1dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze = spark.table(\"workspace.imdb_project_bronze.title_akas\")\n",
    "\n",
    "# Reject rows without titleId\n",
    "reject_null = bronze.filter(F.col(\"titleId\").isNull()) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"titleId is NULL\"))\n",
    "\n",
    "reject_null.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_akas_null_titleId\"\n",
    ")\n",
    "\n",
    "silver = bronze.filter(F.col(\"titleId\").isNotNull()) \\\n",
    "    .withColumn(\"isOriginalTitle_bool\",\n",
    "        F.when(F.col(\"isOriginalTitle\") == \"1\", 1)\n",
    "         .when(F.col(\"isOriginalTitle\") == \"0\", 0)\n",
    "         .otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\"ordering_int\",\n",
    "        F.when(F.col(\"ordering\").rlike(\"^[0-9]+$\"), F.col(\"ordering\").cast(\"int\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.title_akas\"\n",
    ")\n",
    "\n",
    "print(\"Silver title_akas rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.title_akas\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed41a29e-9c17-44a0-be0e-47c3616af75a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_akas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bd11ea-74e8-44cf-a269-0bc76d248bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze = spark.table(\"workspace.imdb_project_bronze.title_crew\")\n",
    "\n",
    "# Reject rows where primary key is missing\n",
    "reject_null = bronze.filter(F.col(\"tconst\").isNull()) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"tconst is NULL\"))\n",
    "\n",
    "reject_null.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_crew_null_tconst\"\n",
    ")\n",
    "\n",
    "silver = bronze.filter(F.col(\"tconst\").isNotNull()) \\\n",
    "    .withColumn(\n",
    "        \"directors_array\",\n",
    "        F.when(F.col(\"directors\").isNotNull(), F.split(F.col(\"directors\"), \",\"))\n",
    "         .otherwise(F.array().cast(\"array<string>\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"writers_array\",\n",
    "        F.when(F.col(\"writers\").isNotNull(), F.split(F.col(\"writers\"), \",\"))\n",
    "         .otherwise(F.array().cast(\"array<string>\"))\n",
    "    )\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.title_crew\"\n",
    ")\n",
    "\n",
    "print(\"Silver title_crew rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.title_crew\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180cdb92-b080-4883-b43d-c335dbd71b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_crew\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb748464-8a6d-4ba8-8ff7-16b04577a884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = spark.table(\"workspace.imdb_project_bronze.title_episode\")\n",
    "\n",
    "# Reject NULL key\n",
    "reject_null = bronze.filter(F.col(\"tconst\").isNull()) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"tconst is NULL\"))\n",
    "\n",
    "reject_null.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_episode_null_tconst\"\n",
    ")\n",
    "\n",
    "silver = bronze.filter(F.col(\"tconst\").isNotNull()) \\\n",
    "    .withColumn(\n",
    "        \"seasonNumber_int\",\n",
    "        F.when(F.col(\"seasonNumber\").rlike(\"^[0-9]+$\"),\n",
    "               F.col(\"seasonNumber\").cast(\"int\"))\n",
    "         .otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"episodeNumber_int\",\n",
    "        F.when(F.col(\"episodeNumber\").rlike(\"^[0-9]+$\"),\n",
    "               F.col(\"episodeNumber\").cast(\"int\"))\n",
    "         .otherwise(None)\n",
    "    )\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.title_episode\"\n",
    ")\n",
    "\n",
    "print(\"Silver title_episode rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.title_episode\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f845af2-c340-413e-85f7-aec6fd2404eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_episode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42e6920-e67b-4ccc-b957-615f58198bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = spark.table(\"workspace.imdb_project_bronze.title_principals\")\n",
    "\n",
    "# Reject if either primary key is missing\n",
    "reject_null = bronze.filter(\n",
    "    F.col(\"tconst\").isNull() | F.col(\"nconst\").isNull()\n",
    ").withColumn(\"reject_reason\", F.lit(\"tconst or nconst is NULL\"))\n",
    "\n",
    "reject_null.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_principals_null_keys\"\n",
    ")\n",
    "\n",
    "silver = bronze.filter(\n",
    "    F.col(\"tconst\").isNotNull() & F.col(\"nconst\").isNotNull()\n",
    ").withColumn(\n",
    "    \"characters_clean\",\n",
    "    F.regexp_replace(\"characters\", r'[\"\\[\\]]', \"\")  # removes [\"\",[]]\n",
    ")\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.title_principals\"\n",
    ")\n",
    "\n",
    "print(\"Silver title_principals rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.title_principals\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954f84af-c83c-42b3-b9c6-2a28f3582686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_principals\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07658adc-679d-4e2d-a7ed-8bdb3844122f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = spark.table(\"workspace.imdb_project_bronze.title_ratings\")\n",
    "\n",
    "# Reject NULL tconst\n",
    "reject_null = bronze.filter(F.col(\"tconst\").isNull()) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"tconst is NULL\"))\n",
    "\n",
    "reject_null.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_ratings_null_tconst\"\n",
    ")\n",
    "\n",
    "clean = bronze.filter(F.col(\"tconst\").isNotNull()) \\\n",
    "    .withColumn(\"averageRating_float\", F.col(\"averageRating\").cast(\"double\")) \\\n",
    "    .withColumn(\"numVotes_int\", F.col(\"numVotes\").cast(\"int\")) \\\n",
    "    .withColumn(\n",
    "        \"invalid_record\",\n",
    "        (F.col(\"averageRating_float\") < 0) |\n",
    "        (F.col(\"averageRating_float\") > 10) |\n",
    "        (F.col(\"numVotes_int\") < 0)\n",
    "    )\n",
    "\n",
    "reject_bad = clean.filter(F.col(\"invalid_record\")) \\\n",
    "    .withColumn(\"reject_reason\", F.lit(\"Invalid rating or numVotes\"))\n",
    "\n",
    "reject_bad.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_rejects.title_ratings_invalid_values\"\n",
    ")\n",
    "\n",
    "silver = clean.filter(~F.col(\"invalid_record\")).drop(\"invalid_record\")\n",
    "\n",
    "silver.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_silver.title_ratings\"\n",
    ")\n",
    "\n",
    "print(\"Silver title_ratings rows:\",\n",
    "      spark.table(\"workspace.imdb_project_silver.title_ratings\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521a7820-ff8c-42b3-8c59-ff32f253433d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM workspace.imdb_project_bronze.title_ratings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff30a27-4f65-4015-bf52-0852b136d61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\"name_basics\",\"title_basics\",\"title_akas\",\n",
    "          \"title_crew\",\"title_episode\",\"title_principals\",\"title_ratings\"]\n",
    "\n",
    "for t in tables:\n",
    "    bronze = spark.table(f\"workspace.imdb_project_bronze.{t}\").count()\n",
    "    silver = spark.table(f\"workspace.imdb_project_silver.{t}\").count()\n",
    "    print(f\"{t}: Bronze={bronze}, Silver={silver}, Dropped={bronze-silver}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1287967e-cefc-4e5a-9862-82b2f4a04fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN workspace.imdb_project_silver;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59a4893-f69d-471e-8c87-73f78138ee4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS imdb_project_gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc2bf3f-4b75-45ba-9ff2-5a27fb6a4fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver = \"workspace.imdb_project_silver\"\n",
    "\n",
    "df = spark.table(f\"{silver}.name_basics\") \\\n",
    "    .select(\n",
    "        \"nconst\",\n",
    "        \"primaryName\",\n",
    "        \"birthYear_clean\",\n",
    "        \"deathYear_clean\",\n",
    "        \"primaryProfession\",\n",
    "        \"knownForTitles\"\n",
    "    ) \\\n",
    "    .dropDuplicates([\"nconst\"])\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.dim_persons\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c624fb-ebd2-4ec6-9955-d479b018d38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_basics\") \\\n",
    "    .select(\n",
    "        \"tconst\",\n",
    "        \"titleType\",\n",
    "        \"primaryTitle\",\n",
    "        \"originalTitle\",\n",
    "        \"isAdult_bool\",\n",
    "        \"startYear_int\",\n",
    "        \"endYear_int\",\n",
    "        \"runtimeMinutes_int\"\n",
    "    )\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.dim_titles\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec00733-9698-4963-8269-031d6c0a7568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_akas\") \\\n",
    "    .select(\"region\") \\\n",
    "    .where(\"region IS NOT NULL\") \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.dim_regions\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e23bc7-5da1-4275-b161-db7716cde72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_akas\") \\\n",
    "    .select(\"language\") \\\n",
    "    .where(\"language IS NOT NULL\") \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.dim_languages\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8652e666-92ea-4e43-8d32-c989bcdfa4f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(f\"{silver}.title_basics\") \\\n",
    "    .select(F.explode(\"genres_array\").alias(\"genre\")) \\\n",
    "    .where(\"genre IS NOT NULL\") \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.dim_genres\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83df820-850d-442e-bad3-ea6b5da85a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_basics\") \\\n",
    "    .select(\"tconst\", F.explode(\"genres_array\").alias(\"genre\"))\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.bridge_title_genres\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61af3ee7-edd6-4238-bc5d-30574369dd5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.name_basics\") \\\n",
    "    .select(\n",
    "        \"nconst\",\n",
    "        F.explode(F.split(\"primaryProfession\", \",\")).alias(\"profession\")\n",
    "    )\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.bridge_person_profession\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f5df2d-f2ba-4826-940a-ee2c1082267e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_akas\") \\\n",
    "    .select(\"titleId\", \"region\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.bridge_title_region\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321fb547-b689-4d80-85fb-80bb59979373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_akas\") \\\n",
    "    .select(\"titleId\", \"language\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.bridge_title_language\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f670d697-58ef-4fda-9cbc-e60709fa1554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_ratings\") \\\n",
    "    .select(\"tconst\", \"averageRating_float\", \"numVotes_int\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.fact_ratings\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd3b77b-15d8-4831-9cb3-ae3c0c14e0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_titles = spark.table(f\"{silver}.title_basics\") \\\n",
    "    .withColumnRenamed(\"load_datetime\", \"title_load_dt\")\n",
    "\n",
    "df_ratings = spark.table(f\"{silver}.title_ratings\") \\\n",
    "    .withColumnRenamed(\"load_datetime\", \"ratings_load_dt\")\n",
    "\n",
    "df = df_titles.join(df_ratings, \"tconst\", \"left\").drop(\"source_file\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.fact_movies\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754cb1c4-aaf4-4e4d-91e2-1ef4c3011551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_principals\") \\\n",
    "    .select(\n",
    "        \"tconst\", \"nconst\", \"category\", \"job\", \"characters_clean\"\n",
    "    )\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.fact_principals\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e6a398-0bdf-4f7f-9671-6b7ce1df93d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{silver}.title_episode\") \\\n",
    "    .select(\n",
    "        \"tconst\",\n",
    "        \"parentTconst\",\n",
    "        \"seasonNumber_int\",\n",
    "        \"episodeNumber_int\"\n",
    "    )\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.imdb_project_gold.fact_episodes\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4956558-e197-48ed-b509-296f41922a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN workspace.imdb_project_gold;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559cb241-3dea-4b6e-986c-c8b07f96cadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_schema = \"workspace.imdb_project_gold\"\n",
    "\n",
    "tables = [\n",
    "    \"dim_persons\",\n",
    "    \"dim_titles\",\n",
    "    \"dim_regions\",\n",
    "    \"dim_languages\",\n",
    "    \"dim_genres\",\n",
    "    \"bridge_title_genres\",\n",
    "    \"bridge_person_profession\",\n",
    "    \"bridge_title_region\",\n",
    "    \"bridge_title_language\",\n",
    "    \"fact_ratings\",\n",
    "    \"fact_movies\",\n",
    "    \"fact_principals\",\n",
    "    \"fact_episodes\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š GOLD LAYER ROW COUNTS\\n\")\n",
    "\n",
    "for t in tables:\n",
    "    try:\n",
    "        count = spark.table(f\"{gold_schema}.{t}\").count()\n",
    "        print(f\"{t:30} â†’ {count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{t:30} â†’ ERROR: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4614257469704009,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "IMDb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
